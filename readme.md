# ğŸš€ Chat LLM com PHP e Streaming Token a Token

Este projeto implementa um **chat em tempo real** utilizando **PHP**, **JavaScript** e **Ollama**, permitindo conversaÃ§Ã£o com modelos de LLM (**Large Language Models**) diretamente em sua mÃ¡quina.

## ğŸ“Œ Recursos
âœ… **Streaming de Respostas Token a Token** (sem usar SSE).  
âœ… **Suporte a Diferentes Modelos (Gemma, DeepSeek, Llama3, etc.).**  
âœ… **Formatador de CÃ³digo com Sintaxe Destacada (Highlight.js).**  
âœ… **Suporte a Markdown (`**negrito**`, `*itÃ¡lico*`, `\`cÃ³digo\``).**  

---
- os crecuros acima sÃ£o importantes ao solicitar cÃ³digo os cÃ³digos serÃ£o formatados para melhor visualizaÃ§Ã£o e estrutura correta, espere o modelo escrever o codigo para a formaÃ§Ã£o ser feita


## âš¡ **PrÃ©-requisitos**
Antes de iniciar o projeto, certifique-se de que tem:
- **[PHP 8+](https://www.php.net/downloads.php)**
- **[Ollama](https://ollama.com/download)**
- **Modelos LLM instalados**

---

## ğŸ“¥ **Instalando o Ollama**
O **Ollama** Ã© um framework para rodar **LLMs localmente**.

- **[Ollama](https://ollama.com/)

Baixando os Modelos de LLM
O Ollama suporta vÃ¡rios modelos. Para este projeto, vocÃª pode escolher entre:

âœ… Baixar o modelo Gemma 2B
```
ollama run  gemma2:2b
```
âœ… Baixar o modelo DeepSeek-R1
```
ollama run deepseek-r1
```
âœ… Baixar o modelo Llama 3
```
ollama run llama3
```

## ğŸš€ Rodando o Servidor Ollama
Antes de iniciar o PHP, o Ollama deve estar rodando em segundo plano.

Isso inicia um servidor local em http://localhost:11434 para processar solicitaÃ§Ãµes do chat.

## ğŸŒ Rodando o Servidor PHP
Agora, inicie o servidor PHP para acessar o chat:

```
php -S localhost:8000
```

Acesse o chat via navegador em http://localhost:8000.

## ğŸ“Œ Tecnologias Utilizadas
- PHP 8+ â†’ Backend para gerenciar requisiÃ§Ãµes do chat.
- JavaScript â†’ ManipulaÃ§Ã£o do DOM e exibiÃ§Ã£o dinÃ¢mica do chat.
- Ollama â†’ Framework para execuÃ§Ã£o de modelos LLM localmente.
- Highlight.js â†’ Destacar sintaxe de cÃ³digo no chat.
- w3-CSS â†’ Para estilizar o frontend.
## ğŸš€ Futuras Melhorias
- Suporte a mais modelos LLM (GPT-4, Mistral, Falcon).
- Cache de respostas para otimizar a performance.
- HistÃ³rico de chat usando banco de dados SQLite.
## ğŸ‘¨â€ğŸ’» Contribuindo
Quer contribuir? Sinta-se Ã  vontade para abrir um Pull Request ou criar uma Issue para melhorias. ğŸš€

## ğŸ“œ LicenÃ§a
Este projeto estÃ¡ licenciado sob a MIT License. Sinta-se livre para usÃ¡-lo e modificÃ¡-lo. ğŸ”¥
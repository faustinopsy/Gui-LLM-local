# üöÄ Chat LLM Local e Offline com PHP, JavaScript, Ollama e LM Studio

Este projeto apresenta uma aplica√ß√£o de **chat em tempo real** que demonstra o consumo de **APIs locais** fornecidas por **servidores de LLM (Large Language Models)** rodando diretamente em sua m√°quina. O foco √© permitir a intera√ß√£o com diferentes modelos **offline, sem necessidade de conex√£o com a internet** ap√≥s a configura√ß√£o inicial.

A aplica√ß√£o utiliza **PHP** no backend para gerenciar as requisi√ß√µes e se comunicar com os servidores LLM locais (Ollama e LM Studio), e **JavaScript** no frontend para a interface de chat din√¢mica e o streaming de respostas.

## ‚ú® Conceito Central

O principal objetivo deste projeto √© ilustrar como √© poss√≠vel **consumir APIs de LLMs que rodam localmente**. Diferentemente de solu√ß√µes que dependem de APIs na nuvem (como a OpenAI), aqui voc√™ controla o ambiente e pode rodar modelos compat√≠veis com **Ollama** ou via a API compat√≠vel com OpenAI do **LM Studio**, alternando entre eles diretamente na aplica√ß√£o. Isso permite **privacidade, uso offline** e a possibilidade de experimentar diversos modelos sem custos de API ou lat√™ncia de rede externa.

## üìå Recursos

‚úÖ **Consumo de APIs Locais:** Interage com servidores LLM rodando no pr√≥prio computador (Ollama e LM Studio).
‚úÖ **Opera√ß√£o Offline:** N√£o requer conex√£o com a internet para o chat ap√≥s a instala√ß√£o dos modelos.
‚úÖ **Suporte Dual Server:** Configurado para alternar entre a API do Ollama (`/api/generate`) e a API compat√≠vel com OpenAI do LM Studio (`/v1/chat/completions`).
‚úÖ **Streaming de Respostas Token a Token:** Exibe a resposta do LLM em tempo real conforme ela √© gerada.
‚úÖ **Suporte a Diferentes Modelos:** Capacidade de interagir com modelos rodando em Ollama (como DeepSeek, Llama3) e modelos rodando em LM Studio (como Gemma 3B IT).
‚úÖ **Formatador de C√≥digo:** Reconhece blocos de c√≥digo (` ``` `) e aplica destaque de sintaxe.
‚úÖ **Suporte a Markdown:** Formata texto em negrito (`**`), it√°lico (`*`) e c√≥digo inline (` ` `).

## ‚ö° Pr√©-requisitos

Para rodar este projeto **como configurado**, voc√™ precisar√° ter os seguintes componentes instalados e rodando em sua m√°quina:

-   **[PHP 8+](https://www.php.net/downloads.php)**: O backend da aplica√ß√£o.
-   **[Ollama](https://ollama.com/download)**: Framework para rodar modelos LLM localmente, usado aqui para modelos como **DeepSeek**, **Llama3**, etc.
-   **[LM Studio](https://lmstudio.ai/)**: Aplica√ß√£o para baixar e rodar modelos LLM localmente, usado aqui especificamente para rodar o modelo **Gemma 3B IT** via sua API compat√≠vel com OpenAI.
-   **Modelos LLM instalados:** Os modelos espec√≠ficos que voc√™ deseja usar, baixados via Ollama e/ou LM Studio.

## üì• Instala√ß√£o e Configura√ß√£o

1.  **Instalar PHP 8+:** Siga as instru√ß√µes no site oficial do PHP para o seu sistema operacional.

2.  **Instalar Ollama:** Baixe e instale o Ollama a partir do link fornecido nos pr√©-requisitos. O Ollama j√° inicia um servidor em `http://localhost:11434` por padr√£o.

3.  **Instalar LM Studio:** Baixe e instale o LM Studio a partir do link fornecido nos pr√©-requisitos. Dentro do LM Studio, inicie o servidor local na porta padr√£o (`http://127.0.0.1:1234`) clicando em "Start Server".

4.  **Baixar Modelos LLM:**
    * **Para Ollama (Ex: DeepSeek, Llama3):** Abra um terminal e utilize o comando `ollama run <nome_do_modelo>`. Por exemplo:
        ```bash
        ollama run deepseek-coder
        ollama run llama3
        ```
        Isso baixar√° e iniciar√° o modelo (voc√™ pode fechar a sess√£o de chat ap√≥s o download).
    * **Para LM Studio (Ex: Gemma 3B IT):** Utilize a interface de busca e download do LM Studio para baixar o modelo desejado, como o `gemma-3b-it`.

## üöÄ Rodando os Servidores LLM

Antes de iniciar a aplica√ß√£o PHP, certifique-se de que **ambos** os servidores, **Ollama** e **LM Studio**, est√£o rodando:

-   **Ollama:** Geralmente roda em segundo plano ap√≥s a instala√ß√£o, servindo a API em `http://localhost:11434`.
-   **LM Studio:** Abra o aplicativo LM Studio e inicie o servidor local (API compat√≠vel com OpenAI) na porta padr√£o `http://127.0.0.1:1234`.

## üåç Rodando o Servidor PHP

Com os servidores LLM locais ativos, inicie o servidor web do PHP no diret√≥rio raiz do projeto:

```bash
php -S localhost:8000